
  # Normalize data:
  data {
    ym <- mean(y)
    ysd <- sd(y)
    for ( i in 1:N ) {
      zy[i] <- ( y[i] - ym ) / ysd
    }
    for ( j in 1:p ) {
      xm[j]  <- mean(x[,j])
      xsd[j] <-   sd(x[,j])
      for ( i in 1:N ) {
        zx[i,j] <- ( x[i,j] - xm[j] ) / xsd[j]
      }
    }
  }
  model {
    # Likelihood 
    for (i in 1:N) {
      y[i] ~ dt( mu[i] , prec , nu )
      mu[i] <- zbeta0 + inprod(x[i,], zbeta)
    } 
    
    # Prior
    zbeta0 ~ dnorm(0, 1/4) # Since data are normalized, they are in [-1, 1]
    
    for(j in 1:p) {
    	zbeta[j] ~ dnorm(0, 1/4)
    }
    
    zsigma ~ dunif( 1.0E-5 , 1.0E+1 )
    prec <- 1 / (zsigma*zsigma)
    
    nu ~ dexp (1/30.0)
    
    #Prediction
    for(t in 1:Ntest){
      yp[t] ~ dt(zbeta0 + inprod(xp[t,], zbeta), prec, nu)
    }
    
    # Transform to original scale:
    beta[1:p] <- ( zbeta[1:p] / xsd[1:p] )*ysd
    beta0 <- zbeta0*ysd  + ym - sum( zbeta[1:p] * xm[1:p] / xsd[1:p] )*ysd
    sigma <- zsigma*ysd
  }
  